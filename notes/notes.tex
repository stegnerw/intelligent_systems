\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}

# Introduction to AI in General

Class about learning machine learning/neural net algorithms
- Implement from scratch
- Focusing on classification

Pattern Recognition:
- Patterns: Spatiotemporal structure
- Programs: Situation specific responses
- Intelligent system should have: Perception, Cognition, and Action
- Sense, Think, Act cycle

Learning: Improving pattern recognition

Intelligence by Design
- Manual creation of the logic and processing

Intelligence by Learning
- Mimic brain-like system that can actually learn (ML)
- Requires lots of data
- Focuses on algorithms

Intelligence by Emergence
- Physical system which learns in the real-world
- More of a full-system than just Learning
- Utilizes physics heavily

These are *not* mutually exclusive

Neural Networks:
- Brain-based view on intelligence algorithms
- Intelligence by Learning system

Problem - Good at pattern recognition, but hard to know they understand

Machine Learning:
- Lots of data/iterations
- Specific goals
- Naive system

Natural Intelligence:
- Very little data/iterations needed
- Internal motivations
- Lifelong learning
- Uses prior knowledge

Naive system - The system starts knowing nothing

Five Fallacies of AI:
- Program-Processor Duality (separation of program and hardware)
- Prescriptive Autonomy (we have to set goals)
- Functional Reductionism (breaking down functions)
- Rationality (optimizing decisions)
- Shallow Adaption (learn intelligence from scratch)

## The Barrier of Meaning

Where does meaning come from?
Check out Melanie Mitchel's report

Braitenberg Vehicles
- Robots with light sensors
- Move based on light
- Can move towards or away from the light
- No real intelligence
- Complex behavior can emerge spontaneously from simple components

## Subsumption
- Combining very simple functions
- Interactions between functions cause complexity
- Doesn't acquire new memories

## Main Lesson
It's all in the networks

Integrate and Fire Model
- *review last lecture*

Izhikevich Model
- Spiking neurons
- Models the shape of action potential
- See slide 20 for equations
- Changing a,b,c,d to make realistic neurons

Simple Neuron Model
- Slide 21 for equations
- Composition function - collects all inputs to "Net Input" (like membrane
  potential)
- Activation function - create output based on Net Input
  - Rate Coding - analog output (sigmoid-type functions)
  - All-or-None Coding - digital output (either 1 or 0)

What do neurons do?
- Discrimination - neuron discriminates between two classes of inputs
  (classification)
- Tuning - neuron is tuned to a specific range of inputs (measurement)
- Spatial discrimination vs temporal discrimination
- Spatiotemporal Discrimination

Rate Coding
- Representing inputs A and B by different rates of spikes

Temporal Coding
- Looking for bursts of spikes, rather than overall rate

Synchrony-Based Coding
- Looking for pairs of input spikes (green apple vs green pepper)

# Network Architectures
- Central Pattern Generators
  - Produce repeating pattern of activity
  - Drive periodic process
  - Breathing, gait
  - Used a lot in bio-inspired robots
- Retinotopic Networks
  - Multiple layers of neurons
  - Maintain spatial relations between layers
  - Early visual system, somatosensory maps
  - Example: Convolutional Neural Networks (CNNs)
- Feed-Forward Networks
  - Layers of neurons
  - Each layer only feeds to the next layer
- Lateral Inhibition/Excitation Network
  - Each neuron inhibits/excites neurons close to it
  - Excites/inhibits further away neurons
  - Projections stay within the layer
  - Bump attractor network - cleans up noise
- Recurrent Networks
  - Connects back to the same layer
- Feed-Forward is passive - no dynamics
- Recurrent depends on previous states

Winner Take All
- Each neuron inhibits all other neurons
- Recurrent structure

# Synaptic Plasticity
- The ability of synapses to change their weight
- Basis of all neural learning

## Hebb Rule
- "Neurons that fire together wire together"
- Increase weight if neurons fire together
- Equation on slide 34
- Weights are proportional to activity in the neurons
- Problem: Cannot decrease weights

## Pre/Post-Synaptically-Gated Hebb Rule
- Slide 37/38
- Add a case where weight decreases when pre/post-synaptic neuron fires without
  post/pre-synaptic neuron

## Spike Time-Dependent Plasticity (STDP)
- Change weight based on timing of spikes
- If post-synaptic spike is after pre-synaptic, decrease weight
- If post-synaptic spike is before pre-synaptic, increase weight

